{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2npMmVzzSM5-"
      },
      "source": [
        "# TP2: MACHINE LEARNING APPLICATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7-DWgNWSM6F"
      },
      "source": [
        "Following the previous nootbook, in this notebook, you will learn the general fundamental of machine learning applied to a classification task which is LGG vs GBM (binary classification). Of course, the aim is not to find the best pipeline with the best algorithm for this task but to understand the principle concept. First, download necessary materials for the afternoon practical sessions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qc9iPlKTFG1"
      },
      "source": [
        "!git clone https://github.com/marsvn/PythonM2-jour3.git\n",
        "%cd PythonM2-jour3/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xr5AtWvSM6L"
      },
      "source": [
        "### Installation of python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDhXnJMmSM6P",
        "scrolled": true
      },
      "source": [
        "!pip install scikit-learn pandas matplotlib numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfe1L4HDSM6j"
      },
      "source": [
        "\n",
        "## General Machine Learning Steps\n",
        "\n",
        "Before we start, let's review the general machine learning steps:\n",
        "\n",
        "1. Data collection, preprocessing (e.g., integration, cleaning, etc.), and exploration;\n",
        "   - Split a dataset into the **training** and **testing** datasets\n",
        "2. Model development:  \n",
        "    A. Assume a model $\\{f\\}$ that is a collection of candidate functions $f$’s (representing posteriori knowledge) we want to discover. Let's assume that each $f$ is parametrized by ${w}$  \n",
        "    B.Define a **cost function** $C({w})$ that measures \"how good a particular $f$ can explain the training data.\" The lower the cost function the better;  \n",
        "3. **Training:** employ an algorithm that finds the best (or good enough) function $f^{*}$ in the model that minimizes the cost function over the training dataset\n",
        "4. **Testing**: evaluate the performance of the learned $f^{*}$ using the testing dataset;\n",
        "5. Apply the model in the real world."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxO1e7MiSM6o"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SS9HJy5SM6r"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "path_dataset = './data/radiomics_analysis_cleaned.csv'\n",
        "\n",
        "data = pd.read_csv(path_dataset)\n",
        "# we will only work with the full area segmentation with all sequences\n",
        "data = data[data['segmentation']=='full']\n",
        "\n",
        "data = data.pivot_table(index=['patient', 'label'],\n",
        "                                columns=['sequence', 'segmentation'],\n",
        "                                values=data.columns[4:])\n",
        "data.columns = ['_'.join(col).strip() for col in data.columns.values]\n",
        "data.reset_index(level=1, inplace=True)\n",
        "\n",
        "display(data)\n",
        "\n",
        "# Convert LGG into class 0 and HGG into class 1\n",
        "data.loc[data['label'] == 'HGG', 'label'] = 1\n",
        "data.loc[data['label'] == 'LGG', 'label'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfw2r-YMSM64"
      },
      "source": [
        "## Splitting datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqgZYh-PSM6-"
      },
      "source": [
        "Let’s now use <code>train_test_split</code> from the function in scikit-learn to divide features data (x_data) and target data (y_data) even further into train and test. Here we will have 30% of the data for the test set. It is also a good practice to define a random state for reproducible results.\n",
        "\n",
        "Note: Stratify parameter in the function makes the equal proportion in the split. For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y_data will make sure that your random split has 25% of 0's and 75% of 1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAbEv0V1SM7C"
      },
      "source": [
        "x_data, y_data = data.drop(columns='label'), data['label'].astype(int).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m0rrTwySM7P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data ,test_size = 0.3, random_state=123, stratify=y_data)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd76_qCqSM7X"
      },
      "source": [
        "Now we have our training set wich is a set for training (and validation). The test set is considered like an unseen set and **will be never seen until the model performance evaluation** – this has to be data that your model hasn’t seen before.\n",
        "\n",
        "But the strategy for evaluate a model depends on your goal and approach.\n",
        "\n",
        " - **Scenario 1: Just train a simple model**  \n",
        "Split the dataset into a separate training and testing set. Train the model on the former, evaluate the model on the latter. Evaluation is done by different performance metrics such as the error, precision, recall, ROC auc, etc.)\n",
        " - **Scenario 2: Train a model and tune (optimize) its hyperparameters.**  \n",
        "Split the dataset into a separate training and validation set. Use techniques such as k-fold cross-validation on the training set to find the “optimal” set of hyperparameters for the model. After the hyperparameter tuning, use the independent test set to get an unbiased estimate of its performance.\n",
        " - **Scenario 3: Build different models and compare algorithms (e.g., SVM vs. logistic regression vs. Random Forests, etc.).**  \n",
        "We use nested or double cross-validation. In nested cross-validation, we have another k-fold cross-validation loop to split the data into training and validation folds. After model selection, the test fold is then used to evaluate the model performance. After the model selection, the testing dataset which was specified above is applied to evaluate our favorite model.\n",
        "\n",
        "<a href=\"https://sebastianraschka.com/faq/docs/evaluate-a-model.html\">\n",
        "  <img src=\"https://github.com/marsvn/PythonM2-jour3/blob/main/images/evaluate_overview.png?raw=1\" alt=\"evaluate_overview\" class=\"center\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDrDriehSM7a"
      },
      "source": [
        "# 1) Introduction to data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWJ3GFuoSM7c"
      },
      "source": [
        "Data pre-processing is an integral step in machine learning because the quality of the data and the useful information that can be derived from it directly affects the ability of our model to learn, so it is extremely important that we pre-process our data before introducing it into our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VDfhJcdSM7g"
      },
      "source": [
        "### Handling Null Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoOngG37SM7l"
      },
      "source": [
        "In any real world dataset there are always few null values. Very few models can handle these NULL or NaN values on its own so we need to intervene. In python NULL is represented with NaN. First of all, we need to check whether we have null values in our dataset or not. We can do that using the isnull() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8ljn4kESM7n"
      },
      "source": [
        "data.isnull().sum() # add .any() if you want to know about if there is any NaN in the sum (returns bool)\n",
        "# Returns the column names along with the number of NaN values in that particular column (we can specify the axis=1, if we want rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3D0LJ6aSM7x"
      },
      "source": [
        "We don't have any NaN for our datas. But there is some strategy to handle the missing datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXDrIYAMSM70"
      },
      "source": [
        "#### 1. The easiest way to solve the problem of NaN is by dropping the rows or columns that contain null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRlSJpc5SM74"
      },
      "source": [
        "data.dropna(); # (axis=0 for columns or 1 for rows), you have various parameters for dropna(), like 'how', 'tresh',\n",
        "# take a look at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGXuiAzgSM8F"
      },
      "source": [
        "#### 2. Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQXghf68SM8K"
      },
      "source": [
        "Imputation is simply the process of substituting the missing values of our dataset. So we can change replace the values by the mean, max, 0, custom function ... Train another algorithm to predict the missing value from the rest of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIOQk6_6SM8N"
      },
      "source": [
        "data.fillna(0); # will replace NaN values by a 0, take a look at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFsJ1qsaSM8W"
      },
      "source": [
        "### Standardization/Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzvchF4BSM8Y"
      },
      "source": [
        "Different radiomics features have different units and range. Some features were designed to fall between 0 and 1, while others have a very large range. In some machine learning algorithms, the objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a wide range of values, the distance will be governed by that particular characteristic. Therefore, the range of all characteristics should be normalized so that each characteristic contributes approximately proportionally to the final distance. We will present the two most common techniques which are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv0_apFPSM9H"
      },
      "source": [
        "#### 1. Min-Max scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adVmLPHySM9K"
      },
      "source": [
        "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR2BWfuOSM9M"
      },
      "source": [
        "$$ X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDPgsDapSM89"
      },
      "source": [
        "Scikit-learn directly implements this for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV5-z_ZcSM9P"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train_standardize = scaler.transform(x_train)\n",
        "# We apply the same transform on the test\n",
        "x_test_standardize = scaler.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRHtAYJ_SM8a"
      },
      "source": [
        "#### 2. Z-Score normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owgtxxi3SM8d"
      },
      "source": [
        "It standardize features by removing the mean and scaling to unit variance. By anothers words, we transform our values such that the mean of the values is 0 and the standard deviation is 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylhGX9VhSM8i"
      },
      "source": [
        "$$ Z = \\frac{x_i - \\mu}{\\sigma} $$  \n",
        "with mean: $$ \\mu = \\frac{1}{N} \\sum_{i=1}^N (x_i) $$\n",
        "and standard deviation: $$ \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CMVvdasSM8l"
      },
      "source": [
        "**Exercice:**\n",
        "Complete the following scripts to:\n",
        "\n",
        "- do the Z-Score normalization on the <code>x_train</code> and apply it on the  <code>x_test</code> without using the scikit-learn function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETwe0kjiSM8n"
      },
      "source": [
        "#@title Exercise 1\n",
        "\n",
        "# How z-score normalization is implemented?\n",
        "\n",
        "import numpy as np\n",
        "import operator\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def z_score(X):\n",
        "    # zero mean and unit variance\n",
        "    mean =  '''CompleteHere'''\n",
        "    std_dev =  '''CompleteHere'''\n",
        "    z =  '''CompleteHere'''\n",
        "    return z, mean, std_dev\n",
        "\n",
        "\n",
        "x_train_standardize, mean, std_dev = '''CompleteHere'''(x_train)\n",
        "x_test_standardize = (('''CompleteHere''' - mean) / std_dev)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXqwOyl4SM9A"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train_standardize = scaler.transform(x_train)\n",
        "# We apply the same transform on the test\n",
        "x_test_standardize = scaler.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUVFMjKiSM9a"
      },
      "source": [
        "There are many other techniques, you can consult this link which\n",
        " <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\">compares the effect of different scalers on data with outliers</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jWYanDHSM9d"
      },
      "source": [
        "# 2) Building models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poBXP9oHSM9q"
      },
      "source": [
        "#### Standard validation (Hold-Out method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY-YNBRnSM9h"
      },
      "source": [
        "Previously we have separated our train and test set datas. The entire train data has been used so far to derive estimates of means and variances for each feature such as to perform Z-score normalization.\n",
        "\n",
        "We now further split the training set into a smaller training set and a validation set. The training set samples will all be used to optimize the parameters of a model (i.e. iteratively converge to a satisfying model from the input family of functions), and the validation set will be used to select a good set of hyper-parameters such as the family of functions to optimize (here, logestic regression, decision trees, support vector machines).\n",
        "\n",
        "The function `train_test_split()` from scikit-learn  is also used on the pair (features, labels) of the training set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqwJKGFKSM9r"
      },
      "source": [
        "<a href=\"https://arxiv.org/pdf/1811.12808.pdf\">\n",
        "  <img src=\"https://github.com/marsvn/PythonM2-jour3/blob/main/images/holdout_method.png?raw=1\" alt=\"holdout_method\" class=\"center\"  height=\"500\" width=\"500\" >\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHDE-6jySM9j"
      },
      "source": [
        "train_features, validation_features, train_labels, validation_labels = \\\n",
        "  train_test_split(x_train, y_train ,test_size = 0.3, random_state=123, stratify=y_train)\n",
        "\n",
        "print('train_features shape: ', train_features.shape)\n",
        "print('validation_features shape: ', validation_features.shape)\n",
        "print('train_labels shape: ', train_labels.shape)\n",
        "print('validation_labels shape: ', validation_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U0d1dX_X8lg"
      },
      "source": [
        "#### Examples of model building with scikit-learn\n",
        "\n",
        "Of the reasons scikit-learn is the leading data science library worldwide is its simplicity of usage and its consistency. By design, the library implements a wide range of machine learning and data processing algorithms, including the most used machine learning families of functions such as logistic regression or boosting algorithms. Here, we will see some examples of machine learning algorithms, including the standard training pipelines using sklearn.\n",
        "\n",
        "As one of the popular classifiers in the medical field applications, logistic regression models the relationship between a categorical response variable $y$ and a set\n",
        "of $x \\in R^k$ of $k$ features per input by fitting a linear equation. The aim is to specify the weights or coefficients in the linear function such that the output be close to the real label.\n",
        "\n",
        "While all of the training process of logistic regression can be coded by hand, its implementation in sklearn lies in the two lines as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX-OZ4MdYuV4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression_model = LogisticRegression()  # instantiate a logistic regression model with default parameters\n",
        "print(logistic_regression_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjehFO2EaVOb"
      },
      "source": [
        "Training is then performed using the `fit` function, which applies gradient descent with input hyper-parameters, and usually stops once a training hyper-parameter is satisfied, such as minimal training error tolerance or number of iterations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKJFqk-Eahp6"
      },
      "source": [
        "clf = logistic_regression_model.fit(X=train_features, y=train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY9TZ2XXtOs3"
      },
      "source": [
        "##### Performance assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0HYYrU0aoFD"
      },
      "source": [
        "Once the model is trained, i.e. `fit` function is done, it can be used to classify any input sample with the correct shape, or vectors of 140 features in this case. Notably, training and validation accuracies can be obtained with other performance indicators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh8ki8RKbBqT"
      },
      "source": [
        "accuracy_train = clf.score(X=train_features, y=train_labels)\n",
        "probs = clf.predict_proba(train_features)\n",
        "accuracy_validation = clf.score(X=validation_features, y=validation_labels)\n",
        "print('Training accuracy', accuracy_train, '; Validation accuracy', accuracy_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGQ4T0VRbbSl"
      },
      "source": [
        "In this case, the dataset contain more positive samples than negative samples, which imply that a model outputting only the positive class, resulting in an accuracy higher than 50%. In other words, the accuracy may not always be suited to the needs of the task in hand. Other performance metrics, such as balanced accuracy (https://en.wikipedia.org/wiki/Precision_and_recall), are implemented in scikit-learn (https://scikit-learn.org/stable/modules/model_evaluation.html). All implemented performance metrics take two vectors as their inputs, one for the predictions of any model and one for ground-truths. We first need to explicitely compute probabilities of the trained model and then run sklearn metrics functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-H1lcRrcOwS"
      },
      "source": [
        "# Use the trained model (clf) to explicitely compute probabilities of all training and validation samples\n",
        "predicted_training_probabilities = clf.predict_proba(train_features)[:, 1]  # for each sample, two outputs probas summing to 1: one for class 0 (LGG), the other for class 1 (HGG)\n",
        "predicted_validation_probabilities = clf.predict_proba(validation_features)[:, 1]  # for each sample, two outputs probas summing to 1: one for class 0 (LGG), the other for class 1 (HGG)\n",
        "\n",
        "# Compute predicted classes from predicte?d probabilities by thresholded probabilities with 0.5: a probability higher than 0.5 would yield HGG prediction, otherwise LGG prediction\n",
        "training_predicted_classes = list(map(lambda proba: int(proba > .5), predicted_training_probabilities))\n",
        "validation_predicted_classes = list(map(lambda proba: int(proba > .5), predicted_validation_probabilities))\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
        "\n",
        "# Metrics function expect first the ground-truth vector, then the predicted probabilities/classes one\n",
        "training_balanced_accuracy = balanced_accuracy_score(train_labels, training_predicted_classes)\n",
        "validation_balanced_accuracy = balanced_accuracy_score(validation_labels, validation_predicted_classes)\n",
        "print('Training balanced accuracy', training_balanced_accuracy, '; Validation balanced accuracy', validation_balanced_accuracy)\n",
        "\n",
        "training_auc = roc_auc_score(train_labels, predicted_training_probabilities)\n",
        "validation_auc = roc_auc_score(validation_labels, predicted_validation_probabilities)\n",
        "print('Training AUC', training_balanced_accuracy, '; Validation AUC', validation_balanced_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2rJh1Zedijk"
      },
      "source": [
        "Although the balanced accuracy alleviate the issue of class imbalance, it is not a rigourous performance assessment of a decision system. Any decision system performance should be assessed using two measures, such as precision and recall. While giant technology companies can make mistakes in online tools such as Facebook suggesting friends tagging on newly uploaded pictures, in medical routine tasks, errors can have a significant impact on patient care or material maintenance. For instance, errors that a patient may be suffering from cancer (false positive) have less impact than false negative that do not detect patients with cancer. The community would expect guarantees that the level of false negative is close to none for most diagnostic tasks, even if the amount of false positive is significant.\n",
        "\n",
        "**Exercice:**\n",
        "Complete the following script to:\n",
        "\n",
        "- evaluate the logistic regression model trained above, using sklearn documentation (https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "- more specifically, compute the precision (also called positive predictive value) and recall (also called true positive rate or sensitivity) called  of the trained classifier on both training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0RlCyKIfIAe"
      },
      "source": [
        "#@title Exercise 2\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "training_precision = '''CompleteHere'''(train_labels, training_predicted_classes)\n",
        "training_recall = '''CompleteHere'''('''CompleteHere''', '''CompleteHere''')\n",
        "\n",
        "validation_precision = precision_score(validation_labels, validation_predicted_classes)\n",
        "validation_recall = recall_score(validation_labels, validation_predicted_classes)\n",
        "\n",
        "print('Training precision', '''CompleteHere''', '; training recall', '''CompleteHere''')\n",
        "print('Validation precision', '''CompleteHere''', '; Validation recall', '''CompleteHere''')\n",
        "\n",
        "#Note: pays attention that the two metrics functions take predicted classes as input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVYrzrz2tuIm"
      },
      "source": [
        "##### Analysing the trained model\n",
        "\n",
        "Fundamentally, logistic regression is parametrized by one parameter per input feature called weight or coefficient plus one parameter, called bias or intercept. The parameters with higher weight have more impact on the output of the system than the ones close to 0.\n",
        "\n",
        "Generally, the parameters of any classifier in scikit-learn can be obtained with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ5H9dzluyXZ"
      },
      "source": [
        "# one parameter per input feature + one for intercept with 0\n",
        "logistic_regression_parameters = clf.coef_\n",
        "logistic_regression_intercept = clf.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeiO7-RYvQmt"
      },
      "source": [
        "Here, we pair each parameter with its associated feature name, then we sort the parameters by magnitude, and retrieve the top 10 with most important magnitude i.e. most important features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le9N0uoUvXZM"
      },
      "source": [
        "# Pair each parameter with its associated feature name\n",
        "logistic_regression_parameters_with_names = list(zip(logistic_regression_parameters[0], x_data.columns.values))\n",
        "print('Example of paires param/feature name', logistic_regression_parameters_with_names[:3])\n",
        "print('Intercept value', logistic_regression_intercept)\n",
        "\n",
        "# Sort paired data with respect to absolute value of parameters\n",
        "logistic_regression_parameters_with_names = sorted(logistic_regression_parameters_with_names, key=lambda pair: abs(pair[0]))\n",
        "\n",
        "print('\\nMost important features:')\n",
        "# Select top 10 max magnitude parameters and print associated feature name\n",
        "for parameter_value, feature_name in logistic_regression_parameters_with_names[:-10:-1]:\n",
        "  print('\\t\\t', feature_name, 'with value:', str(parameter_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z39Ii5dyHrH"
      },
      "source": [
        "At this point, we have trained a logistic regression model, have looked at some metrics for assessing performance, and looked into the trained parameters to infer the most important features found by the model.\n",
        "\n",
        "Usually, we would want to further boost the performance of our decision system. We would like to try different parameters, maybe change logistic regression with another family of functions etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TwavqnXLgq5"
      },
      "source": [
        "### Improving performance with hyper-parameters optimization\n",
        "We saw in the previous section how to implement a logistic regression using scikit-learn for the classification of MRI images into LGG or HGG classes. However, there is no guarantee that the family of functions of logistic regression is the best suited for this task with this type of data. There are a plenty set of families of machine learning decision systems behaving and performing differently given experiment contexts. Therefore, we will look at other families of decision systems on top of logistic regression.\n",
        "More specifically, we will train 6 algorithms on the training set and use the reported generalization performance on the validation set to extract the best performing family of models. We will also look into another hyper-parameter which is the standardization of the data, and compare the validation generalization performance of models trained with standardization with respect to those trained without to determine whether to apply standardization to the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hjg6JXr70Xg"
      },
      "source": [
        "#### Standard validation (Hold-Out method)\n",
        "\n",
        "We observed that how it worked above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw1o-RgfSM98"
      },
      "source": [
        "##### Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCtzGVXNSM9-"
      },
      "source": [
        "# Import librairies\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def standardize(x_train, x_test, type='zscore'):\n",
        "    if type == 'zscore':\n",
        "        scaler = StandardScaler()\n",
        "    else:\n",
        "        scaler = MinMaxScaler()\n",
        "    scaler.fit(x_train)\n",
        "    x_train_standardize = scaler.transform(x_train)\n",
        "    # We apply the same transform on the test\n",
        "    x_test_standardize = scaler.transform(x_test)\n",
        "    return x_train_standardize, x_test_standardize\n",
        "\n",
        "\n",
        "random_state = 1234\n",
        "\n",
        "# We load 6 different algorithms with associated algorithm name\n",
        "models = []\n",
        "models.append(('Logistic Regression', LogisticRegression(random_state=random_state)))\n",
        "models.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n",
        "models.append(('K-nearest neighbours', KNeighborsClassifier()))\n",
        "models.append(('Decision Tree Classifier', DecisionTreeClassifier(random_state=random_state)))\n",
        "models.append(('Gaussian Naive Bayes', GaussianNB()))\n",
        "models.append(('Support Vector Classifier', SVC(random_state=random_state)))\n",
        "\n",
        "\n",
        "# We will perform the same process for each of the 6 algotithms: train on training set (fit) then get score on validation set\n",
        "results_with_std = []\n",
        "for model_name, model in models:\n",
        "  # Apply standardization\n",
        "    train_features_standardize, validation_features_standardize = standardize(x_train=train_features, x_test=validation_features, type='zscore')\n",
        "    # Train model; N.B.: training features not standardized were train_features\n",
        "    clf = model.fit(X=train_features_standardize, y=train_labels)\n",
        "    accuracy_train = clf.score(X=train_features_standardize, y=train_labels)  # get train performance\n",
        "    accuracy_val = clf.score(X=validation_features_standardize, y=validation_labels) # get validation performance\n",
        "    results_with_std.append(accuracy_val)\n",
        "    print(\"%s: train = %.3f, validation = %.3f\" % (model_name, accuracy_train, accuracy_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YJ-dF80R8Ga"
      },
      "source": [
        "Generally, in machine learning, we cannot say which classifier is the best until the implementation is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rw4kpdUSM-M"
      },
      "source": [
        "Here, models have only been tested one time on the validation set. For more robustness let's try a cross validation !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlRPvWjXSM-N"
      },
      "source": [
        "#### K-fold cross validation\n",
        "\n",
        "Regarding the split of data, we have done so far\n",
        "- split our entire dataset into a \"global machine learning optimization\" set and a testing set.\n",
        "- split the \"global machine learning optimization\" set into a training set, used to optimize the parameters of decision systems, and a validation set for hyper-parameters tuning.\n",
        "\n",
        "This second split could have been done randomly and there is no reason to privilege some data samples over others. We could take the \"global machine learning optimization\" set and partition it into a new training set and new associated validation.\n",
        "\n",
        "Cross-validation is the process of simultaneously splitting a set into two sets multiple times, where for each split one set is used to train a model whose performance is computed on the other set. This should yield more robust estimators of performance without relying on more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZzOOWKqSM-T"
      },
      "source": [
        "### Exercice 3\n",
        "Complete the script bellow to:\n",
        "- implement the K-fold cross validation based on https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "- K-fold cross validation is performed as follows:\n",
        "  - Randomly split your entire dataset into k”folds”\n",
        "  - For each k-fold in your dataset, build your model on k – 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n",
        "  - Save the error you see on each of the predictions\n",
        "  - Repeat this until each of the k-folds has served as the test set\n",
        "  - The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orBk-QsxSM-Y"
      },
      "source": [
        "Now, one of the most commonly asked questions is, “How to choose the right value of k?”.\n",
        "\n",
        "Always remember, a lower value of k is more biased, and hence undesirable. On the other hand, a higher value of K is less biased, but can suffer from large variability.\n",
        "\n",
        "A smaller value of k always takes us towards validation set approach, whereas a higher value of k leads to LOOCV approach.\n",
        "\n",
        "Precisely, LOOCV is equivalent to n-fold cross validation where n is the number of training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eikELg6SM-V"
      },
      "source": [
        "<a href=\"https://arxiv.org/pdf/1811.12808.pdf\">\n",
        "  <img src=\"https://github.com/marsvn/PythonM2-jour3/blob/main/images/cross_validation_method.png?raw=1\" alt=\"cross_validation_method\" class=\"center\"  height=\"500\" width=\"500\" >\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqyQPcQgSM-a"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def cross_validation(X, y, model, num_folds=5):\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = X.to_numpy()\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        y = y.to_numpy()\n",
        "\n",
        "    cv = KFold(n_splits=num_folds, random_state=123, shuffle=True)\n",
        "    results_train, results_test = [], []\n",
        "    for train_index, test_index in cv.split(X):\n",
        "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
        "        clf = model.fit('''CompleteHere''', '''CompleteHere''')\n",
        "        accuracy_train = clf.score(X='''CompleteHere''', y='''CompleteHere''')\n",
        "        accuracy_test = clf.score(X='''CompleteHere''', y='''CompleteHere''')  # Return the mean accuracy\n",
        "        results_train.append(accuracy_train)\n",
        "        results_test.append(accuracy_test)\n",
        "    return '''CompleteHere''', '''CompleteHere'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhKThTz3SM-n"
      },
      "source": [
        "We apply a cross validation using the <code>x_train</code> data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srv4tmGhSM_A"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def standardize(x_train, x_test, type='zscore'):\n",
        "    if type == 'zscore':\n",
        "        scaler = StandardScaler()\n",
        "    else:\n",
        "        scaler = MinMaxScaler()\n",
        "    scaler.fit(x_train)\n",
        "    x_train_standardize = scaler.transform(x_train)\n",
        "    # We apply the same transform on the test\n",
        "    x_test_standardize = scaler.transform(x_test)\n",
        "    return x_train_standardize, x_test_standardize\n",
        "\n",
        "\n",
        "def cross_validation_with_standadization(X, y, model, num_folds=5):\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = X.to_numpy()\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        y = y.to_numpy()\n",
        "\n",
        "    cv = KFold(n_splits=num_folds, random_state=123, shuffle=True)\n",
        "    results_train, results_test = [], []\n",
        "    for train_index, test_index in cv.split(X):\n",
        "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
        "\n",
        "         # standardize with z-score\n",
        "        X_train, X_test = standardize(x_train=X_train, x_test=X_test, type='zscore')\n",
        "\n",
        "        clf = model.fit(X_train, y_train)\n",
        "        accuracy_train = clf.score(X=X_train, y=y_train)\n",
        "        accuracy_test = clf.score(X=X_test, y=y_test)  # Return the mean accuracy\n",
        "        results_train.append(accuracy_train)\n",
        "        results_test.append(accuracy_test)\n",
        "    return results_train, results_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIqGmWecSM_H"
      },
      "source": [
        "all_results_cv_train_w_std, all_results_cv_val_w_std, names = [], [], []\n",
        "for name, model in models:\n",
        "    names.append(name)\n",
        "    results_cv_train_w_std, results_cv_val_w_std = cross_validation_with_standadization(X=x_train, y=y_train, model=model, num_folds=5)\n",
        "    all_results_cv_train_w_std.append(results_cv_train_w_std)\n",
        "    all_results_cv_val_w_std.append(results_cv_val_w_std)\n",
        "    means_train, stds_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n",
        "    means_val, stds_val = np.mean(results_cv_val_w_std), np.std(results_cv_val_w_std)\n",
        "    msg = \"%s: train = %.3f (+/- %.3f), test = %.3f (+/- %.3f)\" % (name, means_train, stds_train, means_val, stds_val)\n",
        "    print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVDwmEvxSM_O"
      },
      "source": [
        "Compare the results of cross validation W/O standardization:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"TODO\"\"\""
      ],
      "metadata": {
        "id": "6xIKmvoi-kiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG8--v_2SM_S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "ax = plt.subplot(122)\n",
        "plt.title('Algorithm Comparison using cross validation with preprocessing')\n",
        "plt.boxplot(all_results_cv_val_w_std)\n",
        "ax.set_ylim([0.60, 0.95])\n",
        "ax.set_xticklabels(names, rotation=40)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDeFa8mQSM_X"
      },
      "source": [
        "# 3) Overfitting and Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdQxGDfuSM_a"
      },
      "source": [
        "- **Overfitting**:  \n",
        "A model suffers from Overfitting when it has learned too much from the training data, and does not perform well in practice as a result. This is usually caused by the model having too much exposure to the training data.\n",
        "\n",
        "- **Underfitting**:  \n",
        "A model suffers from Underfitting when it has not learned enough from the training data, and does not perform well in practice as a result. As a direct contrast to the previous idea, this issue is caused by not letting the model learn enough from training data.\n",
        "\n",
        "<a href=\"https://arxiv.org/pdf/1811.12808.pdf\">\n",
        "  <img src=\"https://github.com/marsvn/PythonM2-jour3/blob/main/images/over_and_under_fitting.png?raw=1\" alt=\"over_under_fitting\" class=\"center\"  height=\"400\" width=\"1000\" >\n",
        "</a>\n",
        "\n",
        "*How to detect overfitting?*  \n",
        "A key challenge with overfitting, and with machine learning in general, is that we **do not know how well our model will perform on new data until we actually test it.**\n",
        "\n",
        "To address this, we can split our initial dataset into separate training and test subsets. This method can approximate of how well our model will perform on new data. If our model does much better on the training set than on the test set, then we’re likely overfitting.\n",
        "For example, it would be a big red flag if our model saw 99% accuracy on the training set but only 55% accuracy on the test set.\n",
        "\n",
        "*How to prevent overfitting?*  \n",
        "- **Cross-validation**  \n",
        "The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
        "In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”). Cross-validation allows you to tune hyperparameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n",
        "- **Train with more data**  \n",
        "It will not work every time, but training with more data can help algorithms detect the signal better. Of course, that is not always the case. If we just add more noisy data, this technique won’t help. That is why you should always ensure your data is clean and relevant.\n",
        "- **Remove features**  \n",
        "Some algorithms have built-in feature selection. For those that do not, you can manually improve their generalizability by removing irrelevant input features.\n",
        "- **Regularization**  \n",
        "Regularization refers to a broad range of techniques for artificially forcing your model to be simpler. The method will depend on the type of learner you are using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
        "- **Also, Ensembling, early stopping ...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9falgMUSM_b"
      },
      "source": [
        "# 4) Hyperparameters optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zRJ5ypiSM_d"
      },
      "source": [
        "Wikipedia states that \"hyperparameter tuning/optimizization is choosing a set of optimal hyperparameters for a learning algorithm\".\n",
        "So what is a hyperparameter?\n",
        "<div align=\"center\">\n",
        "    <i> a hyperparameter is a parameter whose value is set before the learning process begins </i>\n",
        "</div>\n",
        "\n",
        "Some examples of hyperparameters include penalty in logistic regression\n",
        "In sklearn, hyperparameters are passed as arguments of the constructor of the models classes.\n",
        "   \n",
        "**Tuning Strategies:**\n",
        " - Grid Search  \n",
        " Also known as an exhaustive search, Grid search looks through each combination of hyperparameters. This means that every combination of specified hyperparameter values will be tried.\n",
        " - Random Search  \n",
        " As its names suggests, Random Search uses random combinations of hyperparameters. This means that not all of the parameter values are tried, and instead, parameters will be sampled with fixed numbers of iterations.\n",
        "\n",
        "<a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\">\n",
        "  <img src=\"https://github.com/marsvn/PythonM2-jour3/blob/main/images/random_grid_search.png?raw=1\" alt=\"random_grid_search class=\"center\"  height=\"400\" width=\"700\" >\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLSSI_hFSM_f"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8BEr4BvSM_h"
      },
      "source": [
        "We define a space for the Grid Search, we will work with z-score preprocessing in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL7AzvrTSM_j"
      },
      "source": [
        "\"\"\"\n",
        "Create a dictionary with classifier name as a key and it's hyper parameters options as a value for grid search\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Logistic Regression Params\n",
        "C = [x for x in np.arange(0.1, 3, 0.2)]\n",
        "penalty = [\"l1\", \"l2\"]\n",
        "fit_intercept = [True, False]\n",
        "solver = [\"saga\"]\n",
        "lr_params = {'C': C,\n",
        "             'penalty': penalty,\n",
        "             'fit_intercept': fit_intercept,\n",
        "             'solver': solver\n",
        "             }\n",
        "\n",
        "# DecisionTreeClassifier PARAMS\n",
        "criterion = ['gini', 'entropy']\n",
        "splitter = ['best', 'random']\n",
        "class_weight = [None, \"balanced\"]\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "max_features = [None, \"sqrt\", \"log2\"]\n",
        "dtc_params = {'criterion': criterion,\n",
        "              'splitter': splitter,\n",
        "              'class_weight': class_weight,\n",
        "              'max_depth': max_depth,\n",
        "              'min_samples_split': min_samples_split,\n",
        "              'min_samples_leaf': min_samples_leaf,\n",
        "              'max_features': max_features\n",
        "              }\n",
        "\n",
        "# KNN PARAMS\n",
        "n_neighbors = [int(x) for x in np.linspace(start=1, stop=20, num=2)]\n",
        "weights = [\"uniform\", \"distance\"]\n",
        "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "leaf_size = [int(x) for x in np.linspace(start=5, stop=50, num=2)]\n",
        "p = [int(x) for x in np.linspace(start=1, stop=4, num=1)]\n",
        "knn_params = {'n_neighbors': n_neighbors,\n",
        "              'weights': weights,\n",
        "              'algorithm': algorithm,\n",
        "              'leaf_size': leaf_size,\n",
        "              'p': p,\n",
        "              }\n",
        "\n",
        "# LDA PARAMS\n",
        "solver = [\"lsqr\"]\n",
        "shrinkage = [\"auto\", None, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "lda_params = {'solver': solver,\n",
        "              'shrinkage': shrinkage\n",
        "              }\n",
        "\n",
        "# GaussianNB PARAMS\n",
        "var_smoothing = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5] #Portion of the largest variance of all features that is added to variances for calculation stability\n",
        "gnb_params = {'var_smoothing': var_smoothing,\n",
        "              }\n",
        "\n",
        "# SVC PARAMS\n",
        "C = [x for x in np.arange(0.1, 2, 0.2)]\n",
        "gamma = [\"auto\"]\n",
        "kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "degree = [1, 2, 3, 4, 5, 6]\n",
        "svc_params = {'C': C,\n",
        "              'gamma': gamma,\n",
        "              'kernel': kernel,\n",
        "              'degree': degree,\n",
        "              }\n",
        "\n",
        "hypertuned_params_gs = {\"Logistic Regression\": lr_params,\n",
        "                     \"Decision Tree Classifier\": dtc_params,\n",
        "                     \"K-nearest neighbours\": knn_params,\n",
        "                     \"Linear Discriminant Analysis\": lda_params,\n",
        "                     \"Gaussian Naive Bayes\": gnb_params,\n",
        "                     \"Support Vector Classifier\": svc_params,\n",
        "                     }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNDsw331SNAD"
      },
      "source": [
        "# grid search function\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def grid_search(X, y, name, model, param_grid, verbose=False):\n",
        "    names = []\n",
        "    names.append(name)\n",
        "    result_gs, max_val_mean_val = [], 0\n",
        "    for i, params in enumerate(param_grid): #enumerate: adds a counter to an iterable object\n",
        "        model = model.set_params(**params)\n",
        "        results_cv_train_w_std, results_cv_val_w_std = cross_validation_with_standadization(X=X, y=y, model=model, num_folds=5)\n",
        "        mean_train, std_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n",
        "        mean_val, std_val = np.mean(results_cv_val_w_std), np.std(results_cv_val_w_std)\n",
        "        if verbose:\n",
        "            print(\"%s - iteration %i: %f (%f)\" % (name, i, mean_val, std_val))\n",
        "        if mean_val > max_val_mean_val:\n",
        "            max_val_mean_test = mean_val\n",
        "            max_val_std_val = std_val\n",
        "            max_val_mean_train = mean_train\n",
        "            max_val_std_train = std_train\n",
        "            max_i = i\n",
        "            best_params = param_grid[i]\n",
        "    msg = \"%s: Maximum value on validation = %.3f (+/- %.3f) with train = %.3f (+/- %.3f) for iteration %i with params: %s\" % (name, max_val_mean_test, max_val_std_val, max_val_mean_train, max_val_std_train, max_i, best_params)\n",
        "    print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "for name, model in models:\n",
        "    param_grid = list(ParameterGrid(hypertuned_params_gs[name]))\n",
        "    grid_search(X=x_train, y=y_train, name=name, model=model, param_grid=param_grid, verbose=False)  # you can set verbose to True to see each iteration"
      ],
      "metadata": {
        "id": "zZ0IZh9OEr3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 4\n",
        "Compare with previous results with the grid search and without any hyperparameters optimization"
      ],
      "metadata": {
        "id": "DibIT5sO_i1Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyaTmT2aSNAT"
      },
      "source": [
        "### Random Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CNO78j2SNAX"
      },
      "source": [
        "We define a space for the Random Search, we will works with z-score preprocessing in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew_T4PGgSNAZ"
      },
      "source": [
        "\"\"\"\n",
        "Create a dictionary with classifier name as a key and it's hyper parameters options as a value for Random search\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Logistic Regression Params\n",
        "# Create regularization hyperparameter distribution using uniform distribution\n",
        "C = uniform(loc=0, scale=4)\n",
        "penalty = [\"l1\", \"l2\"]\n",
        "fit_intercept = [True, False]\n",
        "solver = [\"saga\"]\n",
        "lr_params = {'C': C,\n",
        "             'penalty': penalty,\n",
        "             'fit_intercept': fit_intercept,\n",
        "             'solver': solver\n",
        "             }\n",
        "\n",
        "# DecisionTreeClassifier PARAMS\n",
        "criterion = ['gini', 'entropy']\n",
        "splitter = ['best', 'random']\n",
        "class_weight = [None, \"balanced\"]\n",
        "max_depth = list(range(10, 501))\n",
        "max_depth.append(None)\n",
        "min_samples_split = list(range(2, 101))\n",
        "min_samples_leaf = list(range(1, 50))\n",
        "max_features = [None, \"sqrt\", \"log2\"]\n",
        "dtc_params = {'criterion': criterion,\n",
        "              'splitter': splitter,\n",
        "              'class_weight': class_weight,\n",
        "              'max_depth': max_depth,\n",
        "              'min_samples_split': min_samples_split,\n",
        "              'min_samples_leaf': min_samples_leaf,\n",
        "              'max_features': max_features\n",
        "              }\n",
        "\n",
        "# KNN PARAMS\n",
        "n_neighbors = list(range(1, 101))\n",
        "weights = [\"uniform\", \"distance\"]\n",
        "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "leaf_size = list(range(2, 101))\n",
        "p = list(range(1, 11))\n",
        "knn_params = {'n_neighbors': n_neighbors,\n",
        "              'weights': weights,\n",
        "              'algorithm': algorithm,\n",
        "              'leaf_size': leaf_size,\n",
        "              'p': p,\n",
        "              }\n",
        "\n",
        "# LDA PARAMS\n",
        "solver = [\"lsqr\"]\n",
        "shrinkage = [\"auto\", None, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "lda_params = {'solver': solver,\n",
        "              'shrinkage': shrinkage\n",
        "              }\n",
        "\n",
        "# GaussianNB PARAMS\n",
        "var_smoothing = uniform(loc=0, scale=0.1)\n",
        "gnb_params = {'var_smoothing': var_smoothing,\n",
        "              }\n",
        "\n",
        "# SVC PARAMS\n",
        "C =  uniform(loc=0, scale=2)\n",
        "gamma = [\"auto\"]\n",
        "kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "degree = list(range(1,11))\n",
        "svc_params = {'C': C,\n",
        "              'gamma': gamma,\n",
        "              'kernel': kernel,\n",
        "              'degree': degree,\n",
        "              }\n",
        "\n",
        "hypertuned_params_rs = {\"Logistic Regression\": lr_params,\n",
        "                     \"Decision Tree Classifier\": dtc_params,\n",
        "                     \"K-nearest neighbours\": knn_params,\n",
        "                     \"Linear Discriminant Analysis\": lda_params,\n",
        "                     \"Gaussian Naive Bayes\": gnb_params,\n",
        "                     \"Support Vector Classifier\": svc_params,\n",
        "                     }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11OdRu3WSNAg"
      },
      "source": [
        "# random search function\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def random_search(X, y, name, model, param_grid, nb_iterations, verbose=False):\n",
        "    best_params = []\n",
        "    names = []\n",
        "    names.append(name)\n",
        "    result_rs, max_val_mean_val = [], 0\n",
        "    for i in range(nb_iterations):\n",
        "        # create random param from the grid dict\n",
        "        params = {key: value.rvs() if isinstance(value, type(uniform())) else random.choice(value) for key, value in param_grid.items()}\n",
        "        model = model.set_params(**params)\n",
        "        results_cv_train_w_std, results_cv_val_w_std = cross_validation_with_standadization(X=X, y=y, model=model, num_folds=5)\n",
        "        mean_train, std_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n",
        "        mean_test, std_val = np.mean(results_cv_val_w_std), np.std(results_cv_val_w_std)\n",
        "        if verbose:\n",
        "            print(\"%s - iteration %i: %f (%f)\" % (name, i, mean_test, std_val))\n",
        "        if mean_test > max_val_mean_val:\n",
        "            max_val_mean_test = mean_test\n",
        "            max_val_std_test = std_val\n",
        "            max_val_mean_train = mean_train\n",
        "            max_val_std_train = std_train\n",
        "            max_i = i\n",
        "            best_params = params\n",
        "    msg = \"%s: Maximum value on validation = %.3f (+/- %.3f) with train = %.3f (+/- %.3f) for iteration %i with params: %s\" % (name, max_val_mean_test, max_val_std_test, max_val_mean_train, max_val_std_train, max_i, best_params)\n",
        "    print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_eGlkYCSNAm"
      },
      "source": [
        "for name, model in models:\n",
        "    dic_grid = hypertuned_params_rs[name]\n",
        "    random_search(X=x_train, y=y_train, name=name, model=model, param_grid=dic_grid, nb_iterations=100, verbose=False)  # you can set verbose to True to see each iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOuy4k5wSNA1"
      },
      "source": [
        "Here, we have implemented our own Grid Search and Random Search function to understand the mechanism. Sklearn directly implements the functions, [Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFy8XnzzSNBP"
      },
      "source": [
        "# 5) Ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukGBPl70SNBR"
      },
      "source": [
        "''No free lunch theorem'' states that no machine learning algorithm is universally better than the others in all domains. The goal of ensembling is to combine multiple learner to improve the applicability and get better performance. But always remember, if two models have comparable performance, then you should usually pick the simpler one. [Occam's razor](https://simple.wikipedia.org/wiki/Occam%27s_razor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG8LeKCESNBU"
      },
      "source": [
        "### Voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpuYrDetSNBX"
      },
      "source": [
        "Voting is arguably the most straightforward way to combine multiple learners $d^{(j)}(\\cdot)$. The idea is to take a linear combination of the predictions made by the learners. For example, in multiclass classification, we have\n",
        "$$\\tilde{y}_k =\\sum_j^L w_j d^{(j)}_k(\\boldsymbol{x}), \\text{ where }w_j\\geq 0\\text{ and }\\sum_j w_j=1,$$<p>for any class $k$, where $L$ is the number of voters. This can be simplified to the <strong>plurarity vote</strong> where each voter has the same weight:</p>\n",
        "$$\\tilde{y}_k =\\sum_j \\frac{1}{L} d^{(j)}_k(\\boldsymbol{x}).$$<p> We use the <code>VotingClassifier</code> from Scikit-learn to combine several classifiers.</p>\n",
        "\n",
        "We will use the Sklearn <code>Pipeline</code> tools which allow to combine all the steps we have seen previously\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LICaKWRcSNBY"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipe1 = Pipeline([['sc', StandardScaler()], ['clf', LogisticRegression(**{'C': 0.1, 'fit_intercept': True, 'penalty': 'l1', 'solver': 'saga'}, random_state=random_state)]])\n",
        "pipe2 = Pipeline([['clf', DecisionTreeClassifier(**{'class_weight': None, 'criterion': 'gini', 'max_depth': 80, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'best'}, random_state=random_state)]])\n",
        "pipe3 = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsClassifier(**{'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'})]])\n",
        "pipe4 = Pipeline([['sc', StandardScaler()], ['clf', LinearDiscriminantAnalysis(**{'shrinkage': 0.7, 'solver': 'lsqr'})]])\n",
        "pipe5 = Pipeline([['sc', StandardScaler()], ['clf', GaussianNB(**{'var_smoothing': 1e-09})]])\n",
        "pipe6 = Pipeline([['sc', StandardScaler()], ['clf', SVC(**{'C': 0.5, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'probability': True}, random_state=random_state)]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ARQMF5DSNBe"
      },
      "source": [
        "We can estimate the performance of individual classifiers via the 5-fold CV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPDX2-OASNBj"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "clf_labels = ['LR', 'DTC', 'KNN', 'LDA', 'GNB', 'SVC']\n",
        "print('[Individual]')\n",
        "for pipe, label in zip([pipe1, pipe2, pipe3, pipe4, pipe5, pipe6], clf_labels):\n",
        "    results = cross_validate(estimator=pipe, X=x_train, y=y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
        "    scores_val = results['test_score']\n",
        "    scores_train = results['train_score']\n",
        "    print('%s: train = %.3f (+/- %.3f), validation = %.3f (+/- %.3f)' % (label, scores_train.mean(), scores_train.std(), scores_val.mean(), scores_val.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJotcstcSNBo"
      },
      "source": [
        "We combine the classifiers by <code>VotingClassifer</code> from Scikit-learn and experiment some weight combinations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlu_2z9WSNBp"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "import itertools\n",
        "\n",
        "print('[Voting]')\n",
        "best_vt, best_w, best_val_score, best_train_score = None, (), -1, 1\n",
        "for a, b, c in list(itertools.permutations(range(0,3))): # try some weight combination\n",
        "    clf = VotingClassifier(estimators=[('dtc', pipe2), ('knn', pipe3), ('svc', pipe6)],\n",
        "                           voting='soft', weights=[a,b,c])\n",
        "    results = cross_validate(estimator=clf, X=x_train, y=y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
        "    scores_val = results['test_score']\n",
        "    scores_train = results['train_score']\n",
        "    print('%s: train = %.3f (+/- %.3f), validation = %.3f (+/- %.3f)' % ((a,b,c), scores_train.mean(), scores_train.std(), scores_val.mean(), scores_val.std()))\n",
        "    if best_val_score < scores_val.mean() and best_train_score > scores_train.mean():\n",
        "        best_vt, best_w, best_val_score = clf, (a, b, c), scores_val.mean()\n",
        "\n",
        "print('\\nBest %s: %.3f' % (best_w, best_val_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNd7Zji6SNBu"
      },
      "source": [
        "The best ensemble combines the <code>KNeighborsClassifier</code> and <code>SVC</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gabgxFJFSNBv"
      },
      "source": [
        "# 6) Final prediction and Evaluation metrics ************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIoe_ZlYSNBw"
      },
      "source": [
        "This is the moment you have been waiting for! We will finally be able to evaluate our test set. Our final model will be an ensemble and combines the <code>KNeighborsClassifier </code> and <code>SVC</code> with a z-score features preprocessing. A decision tree does not need a z-score preprocessing because by nature the algorithm is scale invariant. Now, we fit the model on all the training data as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KannU_dNSNBx"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_curve, confusion_matrix, auc, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "pipe3 = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsClassifier(**{'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'})]])\n",
        "pipe6 = Pipeline([['sc', StandardScaler()], ['clf', SVC(**{'C': 0.5000000000000001, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'probability': True})]])\n",
        "\n",
        "clf = VotingClassifier(estimators=[('knn', pipe3), ('svc', pipe6)], voting='soft', weights=[1,2])\n",
        "clf.fit(x_train, y_train)\n",
        "classes = clf.classes_ #number of classes\n",
        "\n",
        "# Here we have the probabilty associate to each classes\n",
        "proba_test = clf.predict_proba(x_test)[:,1] # [:,1] referes to the second class HGG\n",
        "y_pred = np.where(proba_test>0.5, 1, 0) # Here we have the prediction\n",
        "\n",
        "\n",
        "# 1 -- Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 2 -- ROC curve\n",
        "fp_rates, tp_rates, _ = roc_curve(y_test, proba_test, pos_label=1)\n",
        "roc_auc = auc(fp_rates, tp_rates)\n",
        "\n",
        "tn, fp, fn, tp = [i for i in cm.ravel()]\n",
        "\n",
        "# 3 -- Calculate each metrics\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
        "\n",
        "printout = (\n",
        "        f'Precision: {round(precision, 3)} | '\n",
        "        f'Recall: {round(recall, 3)} | '\n",
        "        f'F1 Score: {round(F1, 3)} | '\n",
        "        f'Accuracy Score: {round(accuracy, 3)} | '\n",
        "        f'ROC auc: {round(roc_auc, 3)} | '\n",
        "\n",
        "    )\n",
        "print(printout)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUKDgBwBASrd"
      },
      "source": [
        "Now, we evaluate the classifier based on different metrics.\n",
        "Plot confusion matrix and ROC curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-skpMrqAQ11"
      },
      "source": [
        "ConfusionMatrixDisplay.from_estimator(clf, x_test, y_test);\n",
        "RocCurveDisplay.from_estimator(clf, x_test, y_test);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ExnZBKSNCC"
      },
      "source": [
        "Distribution of predicted probabilty for each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BSCoEchSNCD"
      },
      "source": [
        "df = pd.DataFrame({'probPos': proba_test, 'target': y_test})\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(df[df.target == 0].probPos, density=True, bins=25,\n",
        "             alpha=.5, color='green', label='LGG')\n",
        "plt.hist(df[df.target == 1].probPos, density=True, bins=25,\n",
        "             alpha=.5, color='red', label='HGG')\n",
        "plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n",
        "plt.xlim([0, 1])\n",
        "plt.title('Distributions of Predictions', size=15)\n",
        "plt.xlabel('Positive Probability (predicted)', size=13)\n",
        "plt.ylabel('Samples (normalized scale)', size=13)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}